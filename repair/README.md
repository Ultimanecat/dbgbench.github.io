# Evaluation of Automated Repair Techniques
## Motivation

## Evaluation w/out User Study
Using DBGBench, we can evaluate the *correctness* of the auto-generated patches. We provide the [patches](../patches) generated by the participants, elicit the general fix strategies, and classify them as correct and incorrect. For each incorrect patch, we provide a rationale as to why we classify it as incorrect. For some incorrect patches, we even provide test cases that show their incorrectness.

* To conduct a *plausibility check*, we would execute the existing test suite and the previously failing test cases and confirm that all test cases pass. Concrete steps are provided [here](../patches). For GNU findutils, the developers have constructed a test suite in excess of 1{,}000 test cases. However, plausible patches are not necessarily also correct \cite{plausible}. 
* Hence, to conduct an additional *egression check*, we would constructed test cases for some of the participant-provided, incorrect patches. The execution of this extended test suite allows testing for common mistakes when generating a patch.


## Evaluation w/ User Study
* *Patch correctness* is more realisitically evaluated in user studies where users judge the correctness of the auto-generated patches based on the artifacts in DBGBench.
In such a study, the users can leverage the information provided in DBGBench to decide whether the auto-generated patch is correct.
In essence, DBGBench can significantly reduce the time required for the manual review of auto-generated patches when bug diagnosis, simplified and extended regression test cases, the bug report, the bug diagnosis, fault locations, and developer-provided patches are easily available.
* User studies can also be used to evaluate *patch acceptability*, whether users would accept the auto-generated patches. For instance, Kim et al. \cite{userstudy1} i) asked several students to rate the acceptability of patches generated by \textsc{GenProg}, \textsc{Par}, and a \textsc{Human} for five bugs on a 3-point Likert scale, and ii) asked several students and practitioners to choose the most acceptable from a pair of human-generated and auto-generated patches. Such user studies can leverage DBGBench to extract the correct/incorrect patches provided by professional software engineers. 
* [DBGBench](../dbgbench.summary.pdf) includes the *time taken* by professional software engineers to fix real-world software bugs. 
We can use these timing information to evaluate the *usefulness* of an automated program repair tool. 
To this end, we can design an experiment involving several software professionals and measure the 
reduction in debugging time while using an automated program repair tool. 
